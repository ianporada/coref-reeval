#This is the configuration file for training the LingMess teacher model
seed: 0
output_dir: '$SCRATCH/kd-coref-project-output'
cache_dir: '$SCRATCH/cache/datasets_cache'

preprocessing_cfg:
  cache_dir: ${cache_dir}
  tokenizer_name: microsoft/deberta-xxlarge-v2 #allenai/longformer-large-4096 
  tokenizer_type: not-t5
  max_seq_len: 512
  max_total_seq_len: 5000 
  num_workers: 6

model_cfg:
  llm_pretrained_name: microsoft/deberta-xxlarge-v2 #allenai/longformer-large-4096
  llm_type: not-t5
  max_span_len: 30
  top_lambda: 0.4
  ffnn_size: 2048 #2048
  dropout_prob: 0.3
  num_gpus: 2
  handle_big_llm: True #whether the llm is extremely large and needs to be put on different gpus 
  device_map: {"embeddings":1, "encoder":1, "rel_embeddings":0, "LayerNorm":0, "conv":0} #only used when handle_big_llm is true, describe which gpu each component of llm should be put on

training_cfg:
  num_epochs: 129
  total_steps_per_epoch: 514 #522
  head_lr: 3e-4 #3e-4
  encoder_lr: 1e-5 #1e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_eps: 1e-6
  num_warmup_steps: 5600 #5600

trainer:
  precision: 32
  max_epochs: 129
  default_root_dir: ${output_dir}
  log_every_n_steps: 5