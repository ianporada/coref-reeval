"""
A document processor for producing document contexts
and parsing model outputs.
"""

import logging
from collections import defaultdict
from dataclasses import dataclass
from typing import List, Tuple

import torch

from .formatting_helpers import (clean_mention_part, find_nth_span_in_tokens,
                                 normalize_speaker, parse_actions)

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


@dataclass(frozen=True)
class MentionSpan:
    """Exclusive span, e.g. [local_start, local_end)"""
    sent_id: int
    local_start: int
    local_end: int 


@dataclass
class InputTokens:
    """A list of tokens composing a model input, and a map of these tokens to the original document tokens."""
    tokens: List[str] # list of tokens
    token_map: List[Tuple[int, int]] # each token corresponds to (sent_id, sent_pos) or None


class DocumentProcessor:
    """Iterates through a document converting sentences to Link-Append model inputs."""
    
    def __init__(self, document, tokenizer, max_input_size, no_pound_symbol):
        self.tokenizer = tokenizer
        self.max_input_size = max_input_size
        self.no_pound_symbol = no_pound_symbol
        
        self.id = document['id']
        self.sentences = document['sentences']
        for s in self.sentences:
            s['words'] = [t['text'] for t in s['tokens']]
        
        self.task_prefix = 'coref:' # prefix appended to inputs
        self.current_sentence_index = 0
        
        self.cluster_id_to_spans = defaultdict(list) # clusters are a list of MentionSpans
        # reverse mapping of cluster_id_to_spans
        self.span_to_cluster = {}
        self.span_start_to_spans = defaultdict(list)
        self.span_end_to_spans = defaultdict(list)
        
        self.last_input: InputTokens = None # the last model input generated by the processor
        
        # For generating gold, oracle outputs
        # Saves the mention that most recently occurred in an input for each cluster
        self.hf_cluster_to_last_mention = {} # tuple (cluster_id, sent_id, local_start, local_end) inclusive indices
        self.max_cluster_id = 0 # max cluster id used for an oracle output thus far
        
        self.setup()
        
    def setup(self):
        # normalize all speaker names
        for s in self.sentences:
            s['speaker'] = normalize_speaker(s['speaker'])
        
        self.genre = self.get_genre()
        
        # define the input prefix
        self.prefix_toks = [self.task_prefix, self.genre]
        self.prefix_map = [None] * len(self.prefix_toks)
        self.prefix_tok_ids = self.tokenize(self.prefix_toks)
        self.postfix_tok_ids = self.tokenize([self.tokenizer.eos_token])

    def get_next_model_input(self):
        """Returns the next sentence as a tensor of token ids formatted for the model input.
        E.g. "coref: <genre> # <speaker> <sentence> # <speaker> <sentence> | # <speaker> <sentence> **"
        """
        current_sentence = self.sentences[self.current_sentence_index]
        raw_tokens = current_sentence['tokens']
        speaker, words = current_sentence['speaker'], current_sentence['words']
        
        # input left context
        input: InputTokens = self.get_left_context()
        
        # input sentence
        sent_prefix = ['|', '#', speaker]
        if self.no_pound_symbol:
            sent_prefix = ['|', speaker]
        sent_postfix = ['**']
        sent_tokens = sent_prefix + words + sent_postfix
        sent_token_map = [None] * len(sent_prefix) + \
                        [(self.current_sentence_index, i) for i in range(len(words))] + \
                        [None] * len(sent_postfix)       
        input.tokens += sent_tokens
        input.token_map += sent_token_map
        
        input_tok_ids = self.tokenize(input.tokens)
        
        effective_max_input_size = self.max_input_size - self.prefix_tok_ids.shape[1]
        actual_and_effective_size_difference = input_tok_ids.shape[1] - effective_max_input_size
        
        if actual_and_effective_size_difference >= 0:
            # shorten to max input size if it is too long
            input_tok_ids = input_tok_ids[:, -effective_max_input_size:]
        elif self.current_sentence_index < len(self.sentences) - 1:
            # otherwise pad to the right with future context tokens
            input_tok_ids = torch.cat((input_tok_ids,
                                       self.get_right_context_tok_ids(-1 * actual_and_effective_size_difference)),
                                      dim=1)
        
        # add the prefix and save the last input
        input.tokens = self.prefix_toks + input.tokens
        input.token_map = self.prefix_map + input.token_map
        self.last_input = input
        input_tok_ids = torch.cat((self.prefix_tok_ids, input_tok_ids, self.postfix_tok_ids), dim=1)
        
        logger.debug('Model input: "%s"' % input.tokens)
        
        return input_tok_ids
    
    def update_context(self, output):
        """Given an output string, try to update the context using :func:`parse_output_and_update_clusters()`"""
        assert self.last_input is not None
        
        try:
            self.parse_output_and_update_clusters(output)  
        except Exception as e:
            logging.info('invalid output: "%s"' % output)
            logging.info(e)
            
        self.current_sentence_index += 1
        
    def parse_output_and_update_clusters(self, output):
        """Given an output string, update the left context.
        Output strings are of the form: "<action> ;; <action> ;; <action>" such as
        "<mention> ## <context> -> <mention> ## <context> ;; <mention> ## <context> -> [<cluster_id>"
        E.g.:
        It ## was designated a -> [1 ;; Paris ## , Banks of -> Paris , [2 France ## ] . [1 ;;']"
        """
        if output == 'None [+ E]':
            return
        
        link_actions, append_actions = parse_actions(output)
        
        for reference, cluster in link_actions:
            try:
                reference_span = self.disambiguate_mention(reference, sentence_id=self.current_sentence_index)
            except ValueError as e:
                logging.info(e)
                continue
            self.add_span_to_cluster(cluster, reference_span)
        
        for reference, assignment in append_actions:
            try:
                reference_span = self.disambiguate_mention(reference, sentence_id=self.current_sentence_index)
                assignment_span = self.disambiguate_mention(assignment)
            except ValueError as e:
                logging.info(e)
                continue
            
            if reference_span in self.span_to_cluster:
                # reference span was already assigned to a cluster
                # look for the next match
                occurrence = 2
                while reference_span and reference_span in self.span_to_cluster:
                    try:
                        reference_span = self.disambiguate_mention(reference,
                                                                   occurrence=occurrence,
                                                                   sentence_id=self.current_sentence_index)
                    except ValueError as e:
                        logging.info('Could not find %d occurence of mention "%s"' % (occurrence, reference))
                        reference_span = None
                    occurrence += 1
                    
                if not reference_span:
                    logging.info('All matches for reference span "%s" were already assigned to a cluster' % reference_span)
                    continue
            
            if assignment_span in self.span_to_cluster:
                self.add_span_to_cluster(self.span_to_cluster[assignment_span], reference_span)
                continue
            
            new_cluster = len(self.cluster_id_to_spans) + 1
            self.add_span_to_cluster(new_cluster, reference_span)
            self.add_span_to_cluster(new_cluster, assignment_span)
            
    def get_gold_output(self):
        """Get the next output given the ground truth clusters.
        Must have also been used for generating all previous outputs thus far."""
        assert self.last_input is not None
        
        sentence = self.sentences[self.current_sentence_index]
        clusters = sentence['coref_spans'] # list of [hf_cluster_id, start, end] with inclusive indices
        
        # sort the clusters by when they start (ascending order)
        # when two spans have the same start, process the outer span first
        clusters = sorted(clusters, key=lambda x: (x[1], -x[2])) # (start, -end)
        
        outputs = []
        for cluster in clusters:
            hf_cluster_id, start, end = cluster
            
            # if this is the first mention in the cluster to be processed, skip it
            if hf_cluster_id not in self.hf_cluster_to_last_mention:
                new_last_mention = (-1, self.current_sentence_index, start, end)
                self.hf_cluster_to_last_mention[hf_cluster_id] = new_last_mention
                continue
            
            reference = self.raw_span_to_mention(self.current_sentence_index, start, end)
            
            last_mention = self.hf_cluster_to_last_mention[hf_cluster_id]
            if last_mention[1] == self.current_sentence_index or last_mention[0] == -1: # same sentence
                assignment = self.raw_span_to_mention(last_mention[1], last_mention[2], last_mention[3]) # (sent, start, end)
            else:
                assignment = f'[{last_mention[0]}' # cluster id
            
            cluster_id = last_mention[0]
            if cluster_id < 0:
                self.max_cluster_id += 1
                cluster_id = self.max_cluster_id
                
            new_last_mention = (cluster_id, self.current_sentence_index, start, end)
            self.hf_cluster_to_last_mention[hf_cluster_id] = new_last_mention
            
            outputs.append(f'{reference} -> {assignment}')
            
        gold_output = ' ;; '.join(outputs)
            
        logger.debug('Gold output: "%s"' % gold_output)
        
        return gold_output
    
    def raw_span_to_mention(self, sent_id, local_start, local_end):
        """Find the span as a mention of the form "span ## context" e.g. "I ## am the only"
        Local start and end are inclusive indices."""
        # find the global start and end of the span
        global_start = -1
        global_end = -1 # exclusive index
        for i, pos in enumerate(self.last_input.token_map):
            if not pos or pos[0] != sent_id: # pos is sent id, local id
                continue
            i_local_id = pos[1]
            if i_local_id == local_start:
                global_start = i
            if i_local_id == local_end:
                global_end = i + 1
                break
            
        assert global_start >= 0 and global_end >= 0, (sent_id, local_start, local_end)
        
        context_end = min(len(self.last_input.tokens), global_end + 3)
        
        mention_tokens = self.last_input.tokens[global_start:global_end]
        mention_context_tokens = self.last_input.tokens[global_end:context_end]
        
        mention_str = ' '.join(mention_tokens)
        mention_context_str = ' '.join(mention_context_tokens)
        
        return f'{mention_str} ## {mention_context_str}'

    def get_left_context(self):
        """Return left context as input tokens. Formatted as
        "coref: <genre> # <speaker> <sentence> # <speaker> <sentence>"
        """
        input_toks = []
        input_map = []
        
        # add each sentence
        for sent_id in range(self.current_sentence_index):
            sentence = self.sentences[sent_id]
            
            # add the sentence prefix
            prefix = ['#', sentence['speaker']]
            if self.no_pound_symbol:
                prefix = [sentence['speaker']]
            input_toks += prefix
            input_map += [None] * len(prefix)
            
            # add the tokens from the sentence
            # with special tokens if it is the start or end of a span
            for local_tok_id, token in enumerate(sentence['words']):
                start_of_spans = self.span_start_to_spans[(sent_id, local_tok_id)]
                end_of_spans = self.span_end_to_spans[(sent_id, local_tok_id)]

                # TODO: sort so that 
                # bisect.insort(self.span_start_to_spans[span_start], span, key=lambda x: -x.local_end)
                start_of_spans.sort(key=lambda x: x.local_end, reverse=True)
                
                prefix = []
                for span in start_of_spans:
                    cluster_id = self.span_to_cluster[span]
                    prefix.append(f'[{cluster_id}')
                    
                postfix = [']'] * len(end_of_spans)
                
                input_toks += prefix + [token] + postfix
                input_map += [None] * len(prefix) + \
                        [(sent_id, local_tok_id)] + \
                        [None] * len(postfix)
        
        return InputTokens(tokens=input_toks, token_map=input_map)
    
    def get_right_context_tok_ids(self, max_size):
        """Get the right context token ids up to some max length."""
        right_context_toks = []
        for i in range(self.current_sentence_index + 1, len(self.sentences)):
            next_sentence = self.sentences[i]
            speaker, words = next_sentence['speaker'], next_sentence['words']
            sent_prefix = ['#', speaker]
            if self.no_pound_symbol:
                sent_prefix = [speaker]
            right_context_toks += sent_prefix + words
            if len(right_context_toks) >= max_size:
                break # we can break since the number of words is a lower bound for the number of tokens
        ids = self.tokenize(right_context_toks)
        return ids[:, :min(max_size, ids.shape[1])]
    
    def disambiguate_mention(self, mention_str, occurrence=1, sentence_id=None):
        """Given a string from the last input, return the corresponding MentionSpan.
        Finds the nth occurence of the mention_str"""
        mention_parts = [x.strip().split(' ') for x in mention_str.split('##')]
        mention_parts = [clean_mention_part(x) for x in mention_parts]
        
        if len(mention_parts) != 2:
            raise ValueError('Mention does not have two parts: "%s"' % mention_str)
        
        mention, context = mention_parts
        
        # Find the mention in context
        context_start = find_nth_span_in_tokens(self.last_input, mention + context, occurrence, sentence_id=sentence_id)
        
        if context_start < 0:
            raise ValueError('Mention not found in context: "%s"' % mention_str)
        
        # convert the index into a span
        sent_id = -1
        start = -1
        end = -1
        for i in range(context_start, context_start + len(mention)):
            original_index = self.last_input.token_map[i]
            if original_index is None:
                continue
            original_sent_idx, tok_idx = original_index
            if start < 0:
                sent_id = original_sent_idx
                start = tok_idx
            end = tok_idx
            assert sent_id == original_sent_idx
        
        return MentionSpan(sent_id=sent_id, local_start=start, local_end=end)
        
    def add_span_to_cluster(self, cluster_id, span):
        """Given the given span to the given cluster."""
        if span not in self.cluster_id_to_spans[cluster_id]:
            self.cluster_id_to_spans[cluster_id].append(span)
            
            # also save a reverse mapping
            assert span not in self.span_to_cluster
            self.span_to_cluster[span] = cluster_id
            
            # when multiple spans start at the same token, the span ending last should be written first
            # e.g. [1 [2 hi ] jeff ]
            # so for a given span start, sort the spans by when they end in descending order
            span_start = (span.sent_id, span.local_start)
            # TODO doesnt work with this version of python
            # bisect.insort(self.span_start_to_spans[span_start], span, key=lambda x: -x.local_end)
            # okay for now since spans never have the same start
            self.span_start_to_spans[span_start].append(span)
            
            self.span_end_to_spans[(span.sent_id, span.local_end)].append(span)
    
    def get_genre(self):
        """Take the genre to be the two-letter acronym at the start of the document id."""
        # bc: broadcast conversation
        # bn: broadcast news
        # mz: magazine genre
        # nw: newswire genre
        # pt: pivot text
        # tc: telephone conversation
        # wb: web data
        if str(self.id)[:2] in ['bc', 'bn', 'mz', 'nw', 'pt', 'tc', 'wb']:
            return self.id[:2]
        return 'wb' # default to web data genre
    
    def tokenize(self, tokens):
        return self.tokenizer(' '.join(tokens), return_tensors='pt', add_special_tokens=False).input_ids
    
    def is_finished(self):
        return self.current_sentence_index >= len(self.sentences)
